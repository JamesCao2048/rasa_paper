% !TeX root = ../main.tex

\section{RQ4: Testing Practice Analysis}
\vspace{-3pt}
\label{sec:test_case}

\subsection{Methodology}

To answer \textbf{RQ4}, test cases were inspected in three steps.

\textbf{Step 1: Label test cases.} We manually labeled the granularity level, oracle type and ML stage of each test case.
There are three different granularity levels of test cases: (1) Method-level: testing single or multiple methods; (2) Component-level: testing the complete process of a component in training~or~inference stage; (3) Integration-level: testing the current component with upstream components. 
There are four test oracle types: (1) Given input-output pairs: the input and expected output data are given; (2) Component-specific constraints: the constraints must be satisfied according to the implementation of a component, i.e., the sum of confidence scores of the intent list generated by classifiers should equal to 1; (3) Differential executions:~outputs of executions under different settings should change or remain the same, i.e., given the same input, the outputs of an original ML model and its loaded version from disk should remain the same; (4) Exception: whether or not the test case throws exceptions for certain inputs and configurations. 
Finally, the ML stages covered by test cases include training, inference and evaluation stages.
To test the training stage of a component, test cases must first train it and check whether any test oracle is violated.
Note that there may exist several oracle types and ML stages but only one granularity level for each test case in Rasa.
Two of the authors labeled test cases independently, and the third author was involved to resolve disagreements. The Cohen's Kappa coefficient of granularity level, test oracle and ML stage is 0.907, 0.908, and 0.854, respectively. 

\textbf{Step 2: Collect code coverage of test cases.} We collected the statement coverage and branch coverage of code via \textit{pytest-cov}, because Rasa uses \textit{pytest} to run test cases. 
% The running time of each test case could be extracted from \textit{pytest} running results directly.


\textbf{Step 3: Collect interaction pattern coverage of test~cases.} We injected logging statements into methods of every component, and then executed test cases to collect the co-executed component sets of each test case. 
All component interaction instances, except \textit{Model Inheritance} instances and some \textit{Usage Constraints} instances that cannot be used together, were tried to be matched with these component sets. 
The matched interaction patterns and instances were considered as covered by test cases.
% For interaction patterns that involve multiple components, we tried to match the longest pattern instance. For example, we matched the set {PolicyEnsemble, RulePolicy}, TEDPolicy with the pattern instance (PolicyEnsemble, (TEDPolicy, RulePolicy)) instead of (PolicyEnsemble, TEDPolicy).

\subsection{Results}

% \todo{add test case running time}
  \begin{table*}[ht]
        \begin{center}
        \caption{Code Coverage and Labeled Statistics of Test Cases}
        \vspace{-5pt}
        \begin{tabular}{cccccccccccccc}
          \hline
          \multirow{2}{*}{\textbf{Module}} &
            \multirow{2}{*}{\textbf{Total}} &
            \multicolumn{3}{c}{\textbf{Test Case Type}} &
            \multicolumn{3}{c}{\textbf{Test Case Stage}} &
            \multicolumn{4}{c}{\textbf{Oracle Type}} &
            \multicolumn{2}{c}{\textbf{Code Coverage}} \\ \cline{3-14} 
           &
             &
            Meth. &
            Comp. &
            Integ. &
            Infer. &
            Train &
            Eval. &
            I-O &
            C-S &
            Diff. &
            Exception &
            Stat. Cov. &
            Bran. Cov. \\ \hline
          \multicolumn{1}{c|}{Tokenizer} &
            \multicolumn{1}{c|}{27} &
            7 &
            20 &
            \multicolumn{1}{c|}{0} &
            25 &
            14 &
            \multicolumn{1}{c|}{0} &
            24 &
            1 &
            0 &
            \multicolumn{1}{c|}{3} &
            97.4\% &
            96.8\% \\
          \multicolumn{1}{c|}{Featurizer} &
            \multicolumn{1}{c|}{62} &
            13 &
            14 &
            \multicolumn{1}{c|}{35} &
            56 &
            40 &
            \multicolumn{1}{c|}{0} &
            46 &
            5 &
            3 &
            \multicolumn{1}{c|}{8} &
            95.7\% &
            94.9\% \\
          \multicolumn{1}{c|}{IntentClassifier} &
            \multicolumn{1}{c|}{36} &
            7 &
            15 &
            \multicolumn{1}{c|}{14} &
            29 &
            30 &
            \multicolumn{1}{c|}{0} &
            18 &
            11 &
            7 &
            \multicolumn{1}{c|}{1} &
            92.5\% &
            89.5\% \\
          \multicolumn{1}{c|}{EntityExtractor} &
            \multicolumn{1}{c|}{41} &
            6 &
            19 &
            \multicolumn{1}{c|}{16} &
            34 &
            31 &
            \multicolumn{1}{c|}{0} &
            18 &
            14 &
            8 &
            \multicolumn{1}{c|}{1} &
            92.3\% &
            90.1\% \\
          \multicolumn{1}{c|}{Selector} &
            \multicolumn{1}{c|}{13} &
            4 &
            6 &
            \multicolumn{1}{c|}{3} &
            9 &
            12 &
            \multicolumn{1}{c|}{0} &
            6 &
            5 &
            1 &
            \multicolumn{1}{c|}{1} &
            68.3\% &
            66.4\% \\
          \multicolumn{1}{c|}{Policy} &
            \multicolumn{1}{c|}{165} &
            77 &
            88 &
            \multicolumn{1}{c|}{0} &
            105 &
            127 &
            \multicolumn{1}{c|}{0} &
            117 &
            51 &
            0 &
            \multicolumn{1}{c|}{20} &
            95.7\% &
            94.5\% \\
          \multicolumn{1}{c|}{Shared} &
            \multicolumn{1}{c|}{138} &
            129 &
            2 &
            \multicolumn{1}{c|}{7} &
            90 &
            84 &
            \multicolumn{1}{c|}{0} &
            89 &
            42 &
            2 &
            \multicolumn{1}{c|}{16} &
            92.3\% &
            91.4\% \\
          \multicolumn{1}{c|}{Total} &
            \multicolumn{1}{c|}{461} &
            240 &
            156 &
            \multicolumn{1}{c|}{65} &
            331 &
            317 &
            \multicolumn{1}{c|}{47} &
            312 &
            123 &
            15 &
            \multicolumn{1}{c|}{49} &
            93.2\% &
            92.0\% \\ \hline
          \end{tabular}
        \label{test_case_statistics}
        \end{center}
        \end{table*}

The code coverage and labeled statistics of test cases are shown in Table \ref{test_case_statistics}. 
(1) The total statement coverage and branch coverage of code reach 93.2\% and 92.0\%, which~is~much higher than 21.5\% and 13.3\% in Apollo \cite{pengFirstLookIntegration2020}. 
(2) The coverage of \textit{Selector} is only 68.3\% and 66.4\%, because \textit{Selector} has two candidate ML models while only one of them was tested. 
% (3) The code coverage of \textit{Tokenizer}, \textit{Featurizer} and \textit{Policy} is higher than that of \textit{IntentClassifier}, \textit{EntityExtractor} and \textit{Shared}, since there are more test cases for them per LoC. 
(3) There are 240 (52.0\%) method-level, 156 (33.8\%) component-level and 65 (14.1\%) integration-level test cases. 
(4) There is no integration-level test cases in \textit{Policy}, because \textit{Policy} was tested with given intents and entities input from developers without the dependency of NLU modules. 
(5) Inference and training stages have similar test case quantities.
(6) Only test cases in \textit{Shared} module cover evaluation stage, because \textit{Shared} module provides the evaluation code for all components.
(7) There are 312 (67.7\%), 123 (26.3\%), 15 (3.3\%), and 49 (10.6\%)~test cases with given input-output pairs, component-specific constraints, differential executions and exception test oracles.

        \begin{table}[!t]
            \caption{Test Coverage of Component Interactions}
            \vspace{-10pt}
            \begin{center}
            \tabcolsep=1.0mm
            \begin{tabular}{llll}
            \toprule
            \textbf{Category}
            & \textbf{Sub-Category}
            & \textbf{Cov. Patterns}
            & \textbf{Cov. Instances} \\
            \midrule
            \multirow{5}*{Inter-module}
            &Data Dependency &9/25 &17/95  \\
            &Confidence Checking &0/2 &0/50  \\
            &Output Selection &0/1 &0/18    \\
            &Output Refinement &1/1 & 1/4   \\
            &Usage Constraints &3/3 &3/3    \\
            \midrule
            \multirow{2}*{Intra-module}
            &Functionality Equivalence &2/2 &3/18  \\
            &Prioriy Order &1/1 &4/7  \\
            &Usage Constraints &2/2 &2/4 \\
            \midrule
            Total
            &  &18/37 &30/199 \\
            \bottomrule
            \end{tabular}		
            \label{component_interaction_coverage}
            \end{center}
        \end{table}

        As Table \ref{component_interaction_coverage} shows, the test coverage of interactions~is~relatively low, i.e., 18 (48.6\%) of 37 patterns and 30 (15.1\%)~of~199 instances are covered. This is because only integration-level tests cover components interactions.
        In particular, \textit{Confidence Checking} and \textit{Output Selection} are not covered.
        % and the coverage of \textit{Data Dependency} is very low. because the tests of them is all at method-level and fcuntion-level.
        
        \subsection{Implications} 
        
        \textbf{Low test coverage of component interactions.} It is difficult to achieve a high test coverage of component interactions,~due to the complexity caused by huge configuration space~and~hidden interactions. The only test cases that cover component~interactions (i.e., integration-level test cases) contribute no more than 15\% test cases.
        Yet, integration-level test cases~can~cover~and kill more mutants than component-level and method-level test cases, as many mutants do not manifest~in~non-integration-level test cases \cite{integration_test}.
        Therefore, it is crucial to generate integration-level test cases for ML-enabled systems.
        
        % which can also be helpful to explore the optimal configurations  
        % the metrics of different pipelines given specific task
        % Although the code coverage of test cases in Rasa is relatively high, the interaction coverage is low.
        % only 14.1\% test cases target at  integration-level.  
        
        \textbf{Limited test oracle types.} 
        It is challenging and time-consuming to write test cases with given input-output pairs and component-specific constraints oracles, due to the complexity brought by lacking of specification for interactions. 
        As~a~result, those test cases without the need of specification of interactions, that is, differential executions and exception test oracles, have been widely utilized to tackle the oracle problem~in~test~case generation techniques for traditional software, such as differential testing \cite{evans2007differential}, fuzzing \cite{liang2018fuzzing} and search-based testing \cite{mcminn2011search}.
        Besides, we find that test cases with these two oracles have a similar capability to kill mutants similar to component-specific constraints oracle (see \textbf{RQ5}). In spite of this, only 13.9\% test cases in Rasa are written with differential executions and exception test oracles, implying that there could be a big room to apply these two test oracle types in test case generation techniques for ML-enabled systems.

% The test oracles of 94.0\% test cases are , which are  for system developers to decide. 
% \todo{test case depend on different configurations, caused by complexity in RQ1 but not covered in RQ4. }